{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "fPNNhkBK0VJZFbeN2cKnaV",
     "report_properties": {
      "rowId": "u6LTQPlOHwjW8QonAlWPRw"
     },
     "type": "MD"
    }
   },
   "source": [
    "# Supervised Models with GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "7q43rGAbn0aKlAqAvgCS4n",
     "report_properties": {
      "rowId": "TMyR1M476aRvAtdEJ2lB2o"
     },
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import LinearSVC\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, recall_score, classification_report\n",
    "import xgboost as xgb\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "baaphjw5dZasicIrLM7leU",
     "type": "MD"
    }
   },
   "source": [
    "# Initialize the LSA dataframe from previous steps\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa_category = pd.read_csv(\"SVD_reuters_df.csv\", index_col=0)\n",
    "#lsa_category"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "Cjr9pQBV3ibdnQClk9zHG2",
     "report_properties": {
      "rowId": "LXXrkRCuqPA5ioDjltPr54"
     },
     "type": "MD"
    }
   },
   "source": [
    "## Split the dataframe for Supervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "1kWjOmd2C3SjaJiPonfky1",
     "report_properties": {
      "rowId": "SFuRaLB9ZRvpo7Z2MBoVcy"
     },
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category\n",
      "earn            981\n",
      "acq             573\n",
      "crude            93\n",
      "trade            82\n",
      "money-fx         77\n",
      "interest         68\n",
      "money-supply     38\n",
      "ship             36\n",
      "sugar            31\n",
      "coffee           28\n",
      "gold             23\n",
      "cpi              18\n",
      "gnp              18\n",
      "cocoa            15\n",
      "grain            13\n",
      "reserves         12\n",
      "jobs             12\n",
      "alum             12\n",
      "ipi              11\n",
      "copper           11\n",
      "rubber           10\n",
      "iron-steel        9\n",
      "nat-gas           9\n",
      "bop               8\n",
      "veg-oil           8\n",
      "Name: count, dtype: int64\n",
      "category\n",
      "earn            2942\n",
      "acq             1719\n",
      "crude            281\n",
      "trade            244\n",
      "money-fx         232\n",
      "interest         204\n",
      "money-supply     113\n",
      "ship             108\n",
      "sugar             91\n",
      "coffee            84\n",
      "gold              67\n",
      "gnp               56\n",
      "cpi               53\n",
      "cocoa             46\n",
      "grain             38\n",
      "alum              38\n",
      "reserves          37\n",
      "jobs              37\n",
      "ipi               34\n",
      "copper            33\n",
      "rubber            30\n",
      "iron-steel        29\n",
      "nat-gas           27\n",
      "bop               23\n",
      "veg-oil           22\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "y = lsa_category['category']\n",
    "X = lsa_category.drop(columns=['category'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.25,\n",
    "                                                    random_state=0,\n",
    "                                                    stratify=y)\n",
    "\n",
    "# Make sure classes are balanced after train-test-split\n",
    "print(y_test.value_counts())\n",
    "print(y_train.value_counts())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Models\n",
    "\n",
    "Supervised learning is a type of machine learning where the model is trained on a labeled dataset. In this approach, both the input data (features) and the correct output (labels or responses) are provided during the training process. The aim of a supervised learning algorithm is to learn a mapping function from the input to the output based on the given examples. Once this function is learned, it can be used to predict the output for unseen data.\n",
    "\n",
    "Supervised learning is further classified into two categories: classification and regression. In classification, the output is a category, like 'spam' or 'not spam' for email classification. In regression, the output is a continuous value, like the price of a house. \n",
    "\n",
    "For our project, the models we will be looking at will deal with classification, as we want to predict the labels on the testing data. The models we will be looking into for our project are as follows, with a short descrition of what they are and what they do: \n",
    "\n",
    "1. Random Forest: Random Forest is an ensemble learning method that operates by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) of the individual trees. It's highly accurate, resistant to overfitting, and handles large datasets with high dimensionality well.\n",
    "\n",
    "2. Logistic Regression with LinearSVD: Logistic Regression is a statistical model used for binary classification problems. It estimates the probability of a binary outcome based on one or more independent variables. LinearSVD (Singular Value Decomposition) is a dimensionality reduction method which can be used before applying Logistic Regression to handle high-dimensional data and to avoid overfitting.\n",
    "\n",
    "3. MLP (Multilayer Perceptron) Classifier: This is a type of artificial neural network that is comprised of multiple layers of nodes in a directed graph, with each layer fully connected to the next one. MLPs can model complex functions and solve multi-class classification problems by learning from the training data. They have the ability to learn non-linear models, but require careful tuning of various parameters and can be computationally intensive.\n",
    "\n",
    "4. XGBoost (Extreme Gradient Boosting): XGBoost is an implementation of gradient boosted decision trees designed for speed and performance. It's a powerful approach that's capable of achieving state-of-the-art results on a range of classification problems. It works by sequentially adding new models that correct the errors made by existing models. This method can be used for both binary and multi-class classification problems. It is known for its speed and performance, high accuracy, and ability to handle a variety of data types."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# # define the hyperparameter grid to search over\n",
    "# param_grid = {\n",
    "#     'n_estimators': [50, 100, 200],\n",
    "#     'max_depth': [10, 20, 30],\n",
    "#     'min_samples_split': [2, 5, 10],\n",
    "#     'min_samples_leaf': [1, 2, 4]\n",
    "# }\n",
    "\n",
    "# grid_search = GridSearchCV(rf, param_grid, cv=2, n_jobs=-1)\n",
    "\n",
    "# # fit the model to the training data\n",
    "# grid_search.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_rf = grid_search.best_estimator_\n",
    "# y_pred = best_rf.predict(X_test)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(random_state=42, max_depth= 20, min_samples_leaf= 1, min_samples_split= 2, n_estimators= 200, n_jobs=-1)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_test)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7832422586520947 0.6438568033958397 0.2663282832387027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexa\\mambaforge\\envs\\machinelearning2\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "prec = precision_score(y_test, y_pred, average=\"macro\")\n",
    "recall = recall_score(y_test, y_pred, average=\"macro\")\n",
    "print(accuracy, prec, recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.78\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy score: {:.2f}\".format(accuracy))\n",
    "#print(classification_report(y_test, y_pred))\n",
    "#print(\"Best parameters: {}\".format(grid_search.best_params_))\n",
    "\n",
    "# convert the grid search results to a pandas DataFrame\n",
    "#results = pd.DataFrame(grid_search.cv_results_)\n",
    "#results.to_csv('grid_search_results.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Classification using Linear SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexa\\mambaforge\\envs\\machinelearning2\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "64 fits failed out of a total of 256.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "16 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\alexa\\mambaforge\\envs\\machinelearning2\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\alexa\\mambaforge\\envs\\machinelearning2\\Lib\\site-packages\\sklearn\\svm\\_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "                                           ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\alexa\\mambaforge\\envs\\machinelearning2\\Lib\\site-packages\\sklearn\\svm\\_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\alexa\\mambaforge\\envs\\machinelearning2\\Lib\\site-packages\\sklearn\\svm\\_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "16 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\alexa\\mambaforge\\envs\\machinelearning2\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\alexa\\mambaforge\\envs\\machinelearning2\\Lib\\site-packages\\sklearn\\svm\\_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "                                           ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\alexa\\mambaforge\\envs\\machinelearning2\\Lib\\site-packages\\sklearn\\svm\\_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\alexa\\mambaforge\\envs\\machinelearning2\\Lib\\site-packages\\sklearn\\svm\\_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "16 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\alexa\\mambaforge\\envs\\machinelearning2\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\alexa\\mambaforge\\envs\\machinelearning2\\Lib\\site-packages\\sklearn\\svm\\_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "                                           ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\alexa\\mambaforge\\envs\\machinelearning2\\Lib\\site-packages\\sklearn\\svm\\_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\alexa\\mambaforge\\envs\\machinelearning2\\Lib\\site-packages\\sklearn\\svm\\_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "16 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\alexa\\mambaforge\\envs\\machinelearning2\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\alexa\\mambaforge\\envs\\machinelearning2\\Lib\\site-packages\\sklearn\\svm\\_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "                                           ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\alexa\\mambaforge\\envs\\machinelearning2\\Lib\\site-packages\\sklearn\\svm\\_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\alexa\\mambaforge\\envs\\machinelearning2\\Lib\\site-packages\\sklearn\\svm\\_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\alexa\\mambaforge\\envs\\machinelearning2\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan 0.88995143 0.91287189 0.9127201         nan 0.91818458\n",
      " 0.91211293 0.91196114        nan 0.89632665 0.92501518 0.92501518\n",
      "        nan 0.93047966 0.92516697 0.92486339        nan        nan\n",
      " 0.91196114 0.91196114 0.83029751 0.91818458 0.91196114 0.91287189\n",
      "        nan        nan 0.92486339 0.92516697 0.84805707 0.93047966\n",
      " 0.92516697 0.92516697        nan 0.94960534 0.94945355 0.94960534\n",
      "        nan 0.952034   0.94960534 0.94975713        nan 0.95006072\n",
      " 0.95218579 0.952034          nan 0.95218579 0.95264117 0.952034\n",
      "        nan        nan 0.94975713 0.94945355 0.94019429 0.952034\n",
      " 0.94945355 0.94945355        nan        nan 0.95188221 0.952034\n",
      " 0.93897996 0.95218579 0.95218579 0.95218579        nan 0.94732848\n",
      " 0.94793564 0.94778385        nan 0.94884639 0.94793564 0.9486946\n",
      "        nan 0.94763206 0.94778385 0.94763206        nan 0.94975713\n",
      " 0.94854281 0.94778385        nan        nan 0.94778385 0.94808743\n",
      " 0.94216758 0.94899818 0.94808743 0.94778385        nan        nan\n",
      " 0.94778385 0.94778385 0.94292653 0.94960534 0.94778385 0.94763206\n",
      "        nan 0.94474803 0.9435337  0.94368549        nan 0.94368549\n",
      " 0.94368549 0.94429265        nan 0.9435337  0.94383728 0.94323012\n",
      "        nan 0.94535519 0.94323012 0.94338191        nan        nan\n",
      " 0.9435337  0.9435337  0.93852459 0.94398907 0.9435337  0.9435337\n",
      "        nan        nan 0.94398907 0.94398907 0.93867638 0.94414086\n",
      " 0.94307832 0.94307832]\n",
      "  warnings.warn(\n",
      "c:\\Users\\alexa\\mambaforge\\envs\\machinelearning2\\Lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=2, estimator=LinearSVC(), n_jobs=-1,\n",
       "             param_grid={&#x27;C&#x27;: [0.1, 1, 10, 100], &#x27;dual&#x27;: [True, False],\n",
       "                         &#x27;fit_intercept&#x27;: [True, False],\n",
       "                         &#x27;loss&#x27;: [&#x27;hinge&#x27;, &#x27;squared_hinge&#x27;],\n",
       "                         &#x27;multi_class&#x27;: [&#x27;ovr&#x27;, &#x27;crammer_singer&#x27;],\n",
       "                         &#x27;penalty&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;]})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=2, estimator=LinearSVC(), n_jobs=-1,\n",
       "             param_grid={&#x27;C&#x27;: [0.1, 1, 10, 100], &#x27;dual&#x27;: [True, False],\n",
       "                         &#x27;fit_intercept&#x27;: [True, False],\n",
       "                         &#x27;loss&#x27;: [&#x27;hinge&#x27;, &#x27;squared_hinge&#x27;],\n",
       "                         &#x27;multi_class&#x27;: [&#x27;ovr&#x27;, &#x27;crammer_singer&#x27;],\n",
       "                         &#x27;penalty&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;]})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: LinearSVC</label><div class=\"sk-toggleable__content\"><pre>LinearSVC()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearSVC</label><div class=\"sk-toggleable__content\"><pre>LinearSVC()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=2, estimator=LinearSVC(), n_jobs=-1,\n",
       "             param_grid={'C': [0.1, 1, 10, 100], 'dual': [True, False],\n",
       "                         'fit_intercept': [True, False],\n",
       "                         'loss': ['hinge', 'squared_hinge'],\n",
       "                         'multi_class': ['ovr', 'crammer_singer'],\n",
       "                         'penalty': ['l1', 'l2']})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'loss': ['hinge', 'squared_hinge'],\n",
    "    'dual': [True, False],\n",
    "    'multi_class': ['ovr', 'crammer_singer'],\n",
    "    'fit_intercept': [True, False]\n",
    "}\n",
    "clf = LinearSVC()\n",
    "\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=2, n_jobs=-1)\n",
    "\n",
    "# fit the model to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  {'C': 1, 'dual': True, 'fit_intercept': False, 'loss': 'squared_hinge', 'multi_class': 'crammer_singer', 'penalty': 'l1'}\n",
      "Best score:  0.9526411657559198\n"
     ]
    }
   ],
   "source": [
    "# # Create a LinearSVC object\n",
    "# clf = LinearSVC()\n",
    "\n",
    "# # Train the model on the training set\n",
    "# clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set\n",
    "#y_pred = clf.predict(X_test)\n",
    "\n",
    "best_rf = grid_search.best_estimator_\n",
    "y_pred = best_rf.predict(X_test)    \n",
    "\n",
    "print(\"Best parameters: \", grid_search.best_params_)\n",
    "print(\"Best score: \", grid_search.best_score_)\n",
    "\n",
    "\n",
    "# Evaluate the accuracy of the classifier on the test set\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "# prec = precision_score(y_test, y_pred, average=\"macro\")\n",
    "# recall = recall_score(y_test, y_pred, average=\"macro\")\n",
    "# print(accuracy, prec, recall)\n",
    "# print(\"Accuracy score: {:.2f}\".format(accuracy))\n",
    "# print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP Classifier \n",
    "\n",
    "Since this code took very long, we commented the gridsearch out.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_grid = {\n",
    "#     'hidden_layer_sizes': [(10, 5), (50,), (100,), (50, 50), (100, 100)],\n",
    "#     'activation': ['relu', 'tanh'], \n",
    "#     'max_iter':[100, 500, 1000],\n",
    "#     'alpha': [0.0001, 0.001, 0.01],\n",
    "#     'learning_rate': ['constant', 'adaptive'],\n",
    "# }\n",
    "# grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "# grid_search.fit(X_train, y_train)\n",
    "# best_params = grid_search.best_params_\n",
    "# best_model = grid_search.best_estimator_\n",
    "# accuracy = best_model.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(10, 5), max_iter=1000, random_state=42)\n",
    "\n",
    "# Fit the MLP classifier to the training data\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "# Predict labels on the test data\n",
    "y_pred = mlp.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.81\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         acq       0.85      0.88      0.86       573\n",
      "        alum       0.30      0.25      0.27        12\n",
      "         bop       0.25      0.12      0.17         8\n",
      "       cocoa       0.41      0.60      0.49        15\n",
      "      coffee       0.88      0.50      0.64        28\n",
      "      copper       0.46      0.55      0.50        11\n",
      "         cpi       0.37      0.39      0.38        18\n",
      "       crude       0.77      0.77      0.77        93\n",
      "        earn       0.89      0.94      0.91       981\n",
      "         gnp       0.22      0.22      0.22        18\n",
      "        gold       0.93      0.61      0.74        23\n",
      "       grain       0.21      0.23      0.22        13\n",
      "    interest       0.67      0.46      0.54        68\n",
      "         ipi       0.29      0.18      0.22        11\n",
      "  iron-steel       0.00      0.00      0.00         9\n",
      "        jobs       0.53      0.75      0.62        12\n",
      "    money-fx       0.75      0.62      0.68        77\n",
      "money-supply       0.67      0.76      0.72        38\n",
      "     nat-gas       0.29      0.22      0.25         9\n",
      "    reserves       0.50      0.33      0.40        12\n",
      "      rubber       0.00      0.00      0.00        10\n",
      "        ship       0.50      0.44      0.47        36\n",
      "       sugar       0.76      0.61      0.68        31\n",
      "       trade       0.71      0.74      0.73        82\n",
      "     veg-oil       0.12      0.12      0.12         8\n",
      "\n",
      "    accuracy                           0.81      2196\n",
      "   macro avg       0.49      0.45      0.46      2196\n",
      "weighted avg       0.80      0.81      0.80      2196\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexa\\mambaforge\\envs\\machinelearning2\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning:\n",
      "\n",
      "Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "c:\\Users\\alexa\\mambaforge\\envs\\machinelearning2\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning:\n",
      "\n",
      "Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "c:\\Users\\alexa\\mambaforge\\envs\\machinelearning2\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning:\n",
      "\n",
      "Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy score: {:.2f}\".format(accuracy))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost\n",
    "\n",
    "For XGBoost, we need to label encode the category column, because XGBoost cannot work with labelled columns. Below we initialize the model with some best parameters. Since this code took very long, we commented the gridsearch out.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_grid = {\n",
    "#     'max_depth': [3, 4, 5],\n",
    "#     'learning_rate': [0.1, 0.01, 0.001],\n",
    "#     'n_estimators': [100, 500, 1000]\n",
    "# }\n",
    "\n",
    "# xgb_model = xgb.XGBClassifier()\n",
    "\n",
    "# grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "# grid_search.fit(X_train, y_train)\n",
    "# best_params = grid_search.best_params_\n",
    "# best_model = grid_search.best_estimator_\n",
    "# accuracy = best_model.score(X_test, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = xgb.XGBClassifier(\n",
    "    max_depth=3,\n",
    "    learning_rate=0.1,\n",
    "    n_estimators=100,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = lsa_category['category']\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "X = lsa_category.drop(columns=['category'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.25,\n",
    "                                                    random_state=0,\n",
    "                                                    stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9239526411657559\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.95      0.93       573\n",
      "           1       1.00      0.50      0.67        12\n",
      "           2       1.00      0.38      0.55         8\n",
      "           3       1.00      0.93      0.97        15\n",
      "           4       1.00      0.89      0.94        28\n",
      "           5       1.00      0.82      0.90        11\n",
      "           6       1.00      0.78      0.88        18\n",
      "           7       0.94      0.91      0.93        93\n",
      "           8       0.94      0.98      0.96       981\n",
      "           9       0.93      0.72      0.81        18\n",
      "          10       1.00      0.91      0.95        23\n",
      "          11       0.86      0.46      0.60        13\n",
      "          12       0.88      0.72      0.79        68\n",
      "          13       0.91      0.91      0.91        11\n",
      "          14       1.00      0.33      0.50         9\n",
      "          15       1.00      1.00      1.00        12\n",
      "          16       0.82      0.82      0.82        77\n",
      "          17       0.92      0.95      0.94        38\n",
      "          18       1.00      0.44      0.62         9\n",
      "          19       1.00      0.92      0.96        12\n",
      "          20       1.00      0.80      0.89        10\n",
      "          21       0.78      0.69      0.74        36\n",
      "          22       1.00      0.77      0.87        31\n",
      "          23       0.91      0.95      0.93        82\n",
      "          24       0.80      0.50      0.62         8\n",
      "\n",
      "    accuracy                           0.92      2196\n",
      "   macro avg       0.94      0.76      0.83      2196\n",
      "weighted avg       0.92      0.92      0.92      2196\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xgb_model.fit(X_train, y_train)\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy score: {:.2f}\".format(accuracy))\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "In our study, we evaluated the performance of several supervised machine learning models for our classification task. Our results showed that the Logistic Regression with LinearSVD model performed the best among all the models. It demonstrated a superior ability to handle high-dimensional data and avoid overfitting, providing the most accurate predictions. The XGBoost model followed closely, offering high-speed performance and precise results, but fell short of the top spot due to its relative complexity and need for parameter tuning.\n",
    "\n",
    "The MLP Classifier was the third in our ranking. Despite its ability to model complex, non-linear relationships, the computational intensity and the difficulty in tuning various parameters impacted its overall performance. Finally, the Random Forest model, although robust against overfitting and capable of handling large datasets, performed the least effectively in our particular use-case. This might be attributed to its inability to model complex relationships as effectively as other models in our experiment.\n",
    "\n",
    "These results underscore the importance of understanding the nuances of various machine learning models and choosing the appropriate model based on the specifics of the dataset and the problem at hand. Further tuning and optimization may enhance the performance of these models."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "datalore": {
   "base_environment": "default",
   "computation_mode": "JUPYTER",
   "package_manager": "pip",
   "packages": [
    {
     "name": "nltk",
     "source": "PIP",
     "version": "3.8.1"
    },
    {
     "name": "umap",
     "source": "PIP",
     "version": "0.1.1"
    },
    {
     "name": "umap-learn",
     "source": "PIP",
     "version": "0.5.3"
    }
   ],
   "report_row_ids": [
    "u6LTQPlOHwjW8QonAlWPRw",
    "TMyR1M476aRvAtdEJ2lB2o",
    "Iir7b60B1v22iZarirWbAl",
    "sJabOn0ov4ZjSMq4aEyDCl",
    "HjsgnIODGokPkzsajLQEgS",
    "gNsM6o95VGG4nbvSzdMBon",
    "ScJ0R81K8crwRDJXv8G1jG",
    "h8Msz74vM0OFAR31JPsQlc",
    "N5f92IE0uqWUizQRMMENuB",
    "3n6ntJTkKE1oX1COvJOhZj",
    "ae3yd1HgokN38tbO7tDeiJ",
    "OMCT04A6OyZsO9mFp3p9Rw",
    "utWL8XT1ZTwd5O6zB4p4hS",
    "LXXrkRCuqPA5ioDjltPr54",
    "6AVGn0iWD84qvj59L17YO7",
    "SFuRaLB9ZRvpo7Z2MBoVcy",
    "gBLhVS6bPl5nfOrDzXJClI",
    "Z7caAYctP5scwM7ENXK6lJ",
    "UVzLz6Et9SJX5Yh7nFSk1W"
   ],
   "version": 3
  },
  "kernelspec": {
   "display_name": "machinelearning2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
